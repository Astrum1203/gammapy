.. include:: ../../references.txt

.. _pig-028:

********************************
PIG 28 - Bayesian Inference using Nested Sampling
********************************

* Author: Fabio Acero, RÃ©gis Terrier, Bruno Khelifi
* Created: ``2024-10-14``
* Accepted: ``??``
* Status: Draft
* Discussion: 

Abstract
========

This PIG is intended to introduce a Bayesian Inference class to support nested sampling algorithm to estimate
the marginal evidence and the posterior distributions of all model parameters.

Motivation
==========

(The material below are some notes that I took that can be used later for a tutorial introduction)

Bayesian inference uses prior knowledge, in the form of a prior distribution in order to estimate posterior probabilities which we traditionally visualize in the form of corner plots.
These distributions contains much more information than a single best-fit as they reveal not only the "best model" but the (not always Gaussian) errors and correlation between parameters.

A well know approach to estimate this posterior distribution is the Markov Chain Monte Carlo (MCMC) which uses an ensemble of walkers to produce a chain of samples that after a convergence period will reache a stationary state.
*Once convergence* is reached the successive elements of the chain are samples of the target posterior distribution.
However the weakness of the MCMC approach lies in the *Once convergence* part.
Started far from the best likelihood region, the convergence time can be long or never reached if the walkers fall in a local minima.
The choice of the inititialization point can become critical for complex models with a high number of dimensions and the ability of these walkers to escape a local minimum or to accurately describe a complex likelihood space is not guaranteed.

To overcome these issues, we propose to implement a Bayesian Inference class in gammapy based on the nested sampling (NS) algorithm,
which is a Monte Carlo algorithm for computing an integral of the likelihood function over the model parameter space in 2004 by John Skilling.
The method performs this integral by evolving a collection of points through the parameter space for a recent review : Ashton2022, Buchner2023). 
Without going into too many details, one important specificity of the NS method is that it starts from the entire parameter space and evolves a collection of live points to map all minima (including multiple modes if any)
whereas Markov Chain Monte Carlo methods require an initialization point and the walkers will explore the local likelihood. The ability of these walkers to escape a local minimum or to accurately describe a complex likelihood space is not guaranteed.
This is a fundamental difference between MCMC (and Minuit) which will only ever probe the vicinity along their minimization paths and dot not have an overview of the global likelihood landscape.
The analysis using the NS framework is more CPU time consuming than a standard classical fit but provides the full posterior distribution for all parameters, which is out of reach with traditional fitting techniques (N*(N-1)/2 contour plots to generate).
In addition it is more robust to the choice of initialization, requires less human intervention and is therefore readily pluggable in pipeline analysis.
The following PIG focuses on the implementation of the NS with the UltraNest package, one of the leading package in Astronomy (already used in Cosmology and in X-rays). Another relevant implementation is the one by Dynesty.
As they have very similar API, both packages could be implemented/tested in the future.

Reference : 
- Ultranest : https://johannesbuchner.github.io/UltraNest
- Literature : `Buchner 2023, <http://arxiv.org/abs/2101.09675>`_ , `Ashton 2022, <https://ui.adsabs.harvard.edu/abs/2022NRvMP...2...39A>`_


Use cases
=========

intro here (TBC)

Case 1: Estimate the posterior distribution with a tuning-parameter free approach
---------------------------------------------------------------
Estimating the errors and correlations between the N parameters in a model scales as N*(N-1)/2 and quickly becomes impossible with the regular grid approach of iMinuit.
MCMC methods can provide an approximation of this posterior distribution and have been used for a while in science (dating back to Los Alamos work by Metropolis et al., 1953).
However they still have a few fundamental tuning parameters that require human intervention: 
1. the number of walkers
2. the length of the chain
3. the burn period within that chain (the steps needed to reach convergence)
   
While some methods exists to choose these parameters in an automated way, the user usually ends up choosing by eye the length of the chain and the burn period.
In addition as the MCMC only explores a small fraction of the likelihood landscape, there is no guarantee to reach the global minima.


Case 2: Minuit convergence issues
---------------------------------------------------------------

One of the comment I hear frequently at tutorials, schools or colleagues is not knowing what to do in front of a `Fit failed`` iMinuit message.
This can be due to multiple good reasons (e.g. hitting a boundary) but most commonly it can be related to a starting point far from the solution (in particular the amplitude parameter).
Nested sampling can adress this issue by providing a sparse sampling of a large prior space for the amplitude for example via a `LogUniform` prior.
In addition by providing a full landscape of the likelihood and with no notion of initialization, the model exploration is more robust and with less human intervention.


Case 3: Model comparison using the Bayesian evidence (aka marginal evidence)
---------------------------------------------------------------

replaces the deltaCstat / AIC/BIC, etc 
comparing non nested models


Case 4: Any other suggestions ? 
---------------------------------------------------------------



Implementation impact
==============

The implementation of the Bayesian inference class implies some addition/modifications to the current codebase.

Namely: 

1. Likelihood: The `stat_sum` value needs to be separated into a likelihood term and a prior penalty term.
   While prior will be attached to the model, we want the stat_sum to return only the likelihood and not the penalty term as it is not needed
   for the NS technique. In addition, the NS alogrithm aims to maximize the likelihood (not minimize) and the factor of 2 for the Cstat needs to be removed.
   So likelihood should be : `-0.5*Stat`

2. Priors: The sampling in the NS algorithm is drawn from a "unity cube". This means that all sample are drawn from a [0,1] random uniform distribution that 
 are latter transformed to the parameter space via their inverse cumulative distribution (cdf). An `inverse_cdf` attribute needs to be implemented for all Priors.

1. Sampler: The UltraNest sampler object needs to be initiliazed with the correct parameters and (modified) likelihood 




Proposal
==============



.. code:: python

  class BayesianFit:


            



.. code:: python

  class SamplerLikelihood:


Serialisation of the samples
-------------



Task list 
-----------------------

The PIG implementation will be : 

1.
2.
3.


Decision
========

